# -*- coding: utf-8 -*-
"""MJ colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CZFpDO-xCv6i6K6TMeOuRvd6zIjZURU7

Correlation Matrix
"""

import pandas as pd
import numpy as np
import yfinance as yf
from statsmodels.tsa.stattools import coint

# Step 1: Fetch Historical Prices
def fetch_prices(tickers, start_date, end_date):
    """
    Fetch historical adjusted close prices for the given tickers.
    Filter out tickers with missing data.
    """
    try:
        data = yf.download(tickers, start=start_date, end=end_date)['Close']
        # Drop tickers with all NaN values (missing data)
        data = data.dropna(axis=1, how='all')
        return data
    except Exception as e:
        print(f"Error fetching data: {e}")
        return None

# Define the list of tickers
tickers = ['XLK', 'FDN', 'SMH', 'XT', 'SPY', 'DJI', 'NQX', '^SOX',
           'GOOGL', 'META', 'NVDA', 'CSCO', 'IGM', '^IXIC', 'QQQ', '^GSPC',
           'AAPL', 'MSFT', 'AMZN']

# Define timeframes
timeframes = {
    '2-year': ('2023-03-01', '2025-03-01'),
    '5-year': ('2020-01-01', '2025-01-01'),
    '8-year': ('2017-01-01', '2025-01-01'),
    '10-year': ('2015-01-01', '2025-01-01')
}

# Fetch data for all timeframes
price_data = {}
valid_tickers = set()  # Track valid tickers across all timeframes
for name, (start, end) in timeframes.items():
    data = fetch_prices(tickers, start, end)
    if data is not None:
        price_data[name] = data
        valid_tickers.update(data.columns)

# Print valid tickers
print("\nValid Tickers Found:")
print(valid_tickers)

# Step 2: Calculate Correlation Tables
def calculate_correlation(data):
    """
    Calculate the correlation matrix for the given data.
    """
    return data.corr()

# Generate correlation tables for each timeframe
correlation_tables = {}
for name, data in price_data.items():
    correlation_tables[name] = calculate_correlation(data)

# Print correlation tables to the console
print("\n=== Correlation Tables ===")
for name, corr_table in correlation_tables.items():
    print(f"\n{name} Correlation Table:")
    print(corr_table)

# Step 3: Perform Cointegration Tests
def perform_cointegration_tests(data):
    """
    Perform cointegration tests for all pairs of assets.
    """
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                score, pvalue, _ = coint(data[asset1], data[asset2])
                results.append((asset1, asset2, score, pvalue))
            except Exception as e:
                print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])

# Perform cointegration tests for each timeframe
cointegration_results = {}
for name, data in price_data.items():
    cointegration_results[name] = perform_cointegration_tests(data)

# Print cointegration results to the console
print("\n=== Cointegration Results ===")
for name, results in cointegration_results.items():
    print(f"\n{name} Cointegration Results (p-value < 0.05):")
    significant_pairs = results[results['P-Value'] < 0.05]
    if not significant_pairs.empty:
        print(significant_pairs)
    else:
        print("No significant cointegrated pairs found.")

"""Cointegration Matrix

"""

import pandas as pd
import numpy as np
import yfinance as yf
from statsmodels.tsa.stattools import coint

# Step 1: Fetch Historical Prices
def fetch_prices(tickers, start_date, end_date):
    """
    Fetch historical adjusted close prices for the given tickers.
    Filter out tickers with missing data.
    """
    try:
        data = yf.download(tickers, start=start_date, end=end_date)['Close']
        # Drop tickers with all NaN values (missing data)
        data = data.dropna(axis=1, how='all')
        return data
    except Exception as e:
        print(f"Error fetching data: {e}")
        return None

# Define the list of tickers
tickers = ['XLK', 'FDN', 'SMH', 'XT', 'SPY', 'DJI', 'NQX', '^SOX',
            'GOOGL', 'META', 'NVDA', 'CSCO', 'IGM', '^IXIC', 'QQQ', '^GSPC',
            'AAPL', 'MSFT', 'AMZN']

# Define timeframes
timeframes = {
    '2-year': ('2023-03-01', '2025-03-01'),
    '5-year': ('2020-01-01', '2025-01-01'),
    '8-year': ('2017-01-01', '2025-01-01'),
    '10-year': ('2015-01-01', '2025-01-01')
}

# Fetch data for all timeframes
price_data = {}
valid_tickers = set()  # Track valid tickers across all timeframes
for name, (start, end) in timeframes.items():
    data = fetch_prices(tickers, start, end)
    if data is not None:
        price_data[name] = data
        valid_tickers.update(data.columns)

# Print valid tickers
print("\nValid Tickers Found:")
print(valid_tickers)

# Step 2: Perform Cointegration Tests (replacing correlation)
def perform_cointegration_matrix(data):
    """
    Perform cointegration tests for all pairs of assets and create a matrix.
    """
    n = len(data.columns)
    matrix = pd.DataFrame(index=data.columns, columns=data.columns)
    for i in range(n):
        for j in range(n):
            if i == j:
                matrix.iloc[i, j] = 1.0  # Cointegration with itself is perfect
            else:
                asset1 = data.columns[i]
                asset2 = data.columns[j]
                try:
                    _, pvalue, _ = coint(data[asset1], data[asset2])
                    matrix.iloc[i, j] = pvalue
                except Exception as e:
                    print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
                    matrix.iloc[i, j] = np.nan
    return matrix

# Generate cointegration matrices for each timeframe
cointegration_matrices = {}
for name, data in price_data.items():
    cointegration_matrices[name] = perform_cointegration_matrix(data)

# Print cointegration matrices to the console
print("\n=== Cointegration Matrices ===")
for name, matrix in cointegration_matrices.items():
    print(f"\n{name} Cointegration Matrix (p-values):")
    print(matrix)

# Step 3: Perform Cointegration Tests (for significant pairs)
def perform_cointegration_tests_significant(data):
    """
    Perform cointegration tests for all pairs of assets.
    """
    n = len(data.columns)
    results = []
    for i in range(n):
        for j in range(i + 1, n):
            asset1 = data.columns[i]
            asset2 = data.columns[j]
            try:
                score, pvalue, _ = coint(data[asset1], data[asset2])
                results.append((asset1, asset2, score, pvalue))
            except Exception as e:
                print(f"Error performing cointegration test for {asset1} and {asset2}: {e}")
    return pd.DataFrame(results, columns=['Asset1', 'Asset2', 'Cointegration Score', 'P-Value'])

# Perform cointegration tests for each timeframe
cointegration_results = {}
for name, data in price_data.items():
    cointegration_results[name] = perform_cointegration_tests_significant(data)

# Print cointegration results to the console
print("\n=== Cointegration Results (Significant Pairs) ===")
for name, results in cointegration_results.items():
    print(f"\n{name} Cointegration Results (p-value < 0.05):")
    significant_pairs = results[results['P-Value'] < 0.05]
    if not significant_pairs.empty:
        print(significant_pairs)
    else:
        print("No significant cointegrated pairs found.")

import pandas as pd
import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.stattools import adfuller


# Load CSVs (update the file names accordingly)
asset1 = pd.read_csv("/content/CAPITALCOM_US100, 1D_73550.csv")
asset2 = pd.read_csv("/content/NASDAQ_NQX, 1D_9a012.csv")

# Rename columns for clarity
asset1 = asset1.rename(columns={'time': 'Date', 'close': 'Close1'})
asset2 = asset2.rename(columns={'time': 'Date', 'close': 'Close2'})

# Convert to datetime
asset1['Date'] = pd.to_datetime(asset1['Date'])
asset2['Date'] = pd.to_datetime(asset2['Date'])

# Merge on Date
df = pd.merge(asset1[['Date', 'Close1']], asset2[['Date', 'Close2']], on='Date')

# Compute spread
df['Spread'] = df['Close1'] - df['Close2']
spread = df['Spread']
# Rolling window (change 30 to your preference)
window = 60
df['Spread_Mean'] = df['Spread'].rolling(window=window).mean()
df['Spread_Std'] = df['Spread'].rolling(window=window).std()

# Compute Z-Score
df['Z_Score'] = (df['Spread'] - df['Spread_Mean']) / df['Spread_Std']
print(df['Z_Score'])





def check_stationarity(spread):
    """
    Checks if a spread is stationary using the ADF test.
    """
    result = adfuller(spread.dropna())  # Drop NaN values
    print('ADF Statistic:', result[0])
    print('p-value:', result[1])
    print('Critical Values:', result[4])

    if result[1] <= 0.05:
        print("Spread is stationary (reject H0).")
        return True
    else:
        print("Spread is non-stationary (fail to reject H0).")
        return False

# Example usage (assuming 'df' is your DataFrame with the 'Spread' column)
is_stationary = check_stationarity(df['Spread'])

# Plot Z-Score
plt.figure(figsize=(14, 6))
plt.plot(df['Date'], df['Z_Score'], label='Z-Score', color='orange')
plt.axhline(0, color='gray', linestyle='--')
plt.axhline(1.5, color='green', linestyle='--', label='Upper Threshold (+1.5)')
plt.axhline(-1.5, color='red', linestyle='--', label='Lower Threshold (-1.5)')
plt.title("Z-Score of Spread Between Asset1 and Asset2")
plt.xlabel("Date")
plt.ylabel("Z-Score")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()



import yfinance as yf
import pandas as pd
from statsmodels.tsa.stattools import coint
from scipy.stats import pearsonr
import numpy as np

# Define the assets
assets = ['XLK', 'FDN', 'SMH', 'XT', 'SPY', 'DJINET', 'NQ', 'NQX', 'SOX', 'GOOGL', 'FB', 'NVDA', 'CSCO', 'VIT', 'IGM', 'IXIC', 'GSPT', 'AAPL', 'MSFT', 'AMZN']

# Fetch the prices of the assets
prices = {}
for asset in assets:
    prices[asset] = yf.download(asset, start='2010-01-01', end='2022-02-26')['Close']

# Create a DataFrame with the prices
# The original line df = pd.DataFrame(prices) caused the error
# Instead, we pass prices to the 'data' argument and specify an index
# using the dates from one of the price Series (e.g., prices['XLK']).
# The issue was that 'prices' was a dictionary of Series, and pandas
# expected a list or a 1-dimensional array. We fix this by constructing
# the DataFrame from the dictionary of Series directly.
df = pd.DataFrame(prices)

# Calculate the correlation tables for different time periods
correlation_tables = {}
for period in [2, 5, 8, 10]:
    start_date = '20' + str(22 - period) + '-01-01'
    end_date = '2022-02-26'
    df_period = df[start_date:end_date]
    correlation_table = df_period.corr()
    correlation_tables[period] = correlation_table

# Print the correlation tables
for period, correlation_table in correlation_tables.items():
    print(f'Correlation Table for {period}-year period:')
    print(correlation_table)
    print()

# Perform cointegration tests for pair trading strategy
for i in range(len(assets)):
    for j in range(i + 1, len(assets)):
        asset1 = assets[i]
        asset2 = assets[j]
        df_asset1 = df[asset1]
        df_asset2 = df[asset2]
        score, pvalue, _ = coint(df_asset1, df_asset2)
        if pvalue < 0.05:
            print(f'Cointegration detected between {asset1} and {asset2} (p-value: {pvalue})')
        else:
            print(f'No cointegration detected between {asset1} and {asset2} (p-value: {pvalue})')

import yfinance as yf
import pandas as pd
import itertools
from statsmodels.tsa.stattools import coint
import datetime

# Define asset list
assets = [
    "XLK", "FDN", "SMH", "XT", "SPY", "DJINET", "NQ", "NQX", "SOX",
    "GOOGL", "META", "NVDA", "CSCO", "VIT", "IGM", "IXIC", "GSPT",
    "AAPL", "MSFT", "AMZN"
]

# Replace any deprecated tickers
ticker_map = {"FB": "META"}
assets = [ticker_map.get(t, t) for t in assets]

# Set date ranges
end_date = datetime.datetime.today()
ranges = {
    "2Y": end_date - datetime.timedelta(days=2 * 365),
    "5Y": end_date - datetime.timedelta(days=5 * 365),
    "8Y": end_date - datetime.timedelta(days=8 * 365),
    "10Y": end_date - datetime.timedelta(days=10 * 365)
}

# Download historical data and calculate correlation matrices
correlation_tables = {}
all_data = {}

for label, start_date in ranges.items():
    data = yf.download(assets, start=start_date.strftime("%Y-%m-%d"), end=end_date.strftime("%Y-%m-%d"))["Adj Close"]
    data = data.dropna(axis=1, how='any')  # Drop tickers with missing data
    all_data[label] = data
    correlation_tables[label] = data.pct_change().corr()

# Show correlation matrices
for label, corr_df in correlation_tables.items():
    print(f"\n--- {label} Correlation Table ---")
    print(corr_df.round(2))

# Cointegration Test (2-Year Data)
print("\n--- Cointegration Test Results (2-Year Data) ---")
data_2y = all_data["2Y"]
results = []

for a, b in itertools.combinations(data_2y.columns, 2):
    score, pvalue, _ = coint(data_2y[a], data_2y[b])
    results.append((f"{a} - {b}", pvalue))

# Sort and display cointegrated pairs
results_df = pd.DataFrame(results, columns=["Pair", "P-Value"])
results_df["Cointegrated"] = results_df["P-Value"] < 0.05
results_df = results_df.sort_values("P-Value")
print(results_df)

!pip install ta
!pip install pandas


import psycopg2
import pandas as pd
import ta

database_username = 'devuser'
database_password = 'admin'
database_host = "51.81.60.92"
database_port = '5433'
database_name = 'devdb'


def fetch_data_from_table(table_name, start_time, end_time):
    connection = psycopg2.connect(
        user=database_username,
        password=database_password,
        host=database_host,
        port=database_port,
        database=database_name
    )

    try:
        query = f"SELECT * FROM {table_name} WHERE timestamp >= '{start_time}' AND timestamp <= '{end_time}'"
        df = pd.read_sql_query(query, connection)
        return df
    finally:
        connection.close()
def calculate_indicators(df):
    # Calculate EMAs
    df['ema7'] = ta.trend.ema_indicator(df['close'], window=7)
    df['ema14'] = ta.trend.ema_indicator(df['close'], window=14)
    df['ema20'] = ta.trend.ema_indicator(df['close'], window=20)
    df['ema50'] = ta.trend.ema_indicator(df['close'], window=50)
    df['ema100'] = ta.trend.ema_indicator(df['close'], window=100)
    df['ema200'] = ta.trend.ema_indicator(df['close'], window=200)

    # Calculate RSI
    df['rsi'] = ta.momentum.rsi(df['close'], window=14)

    # Calculate MFI
    df['mfi'] = ta.volume.money_flow_index(df['high'], df['low'], df['close'], df['volume'], window=14)

    # Calculate SMI
    df['smi'] = ta.momentum.stoch(df['high'], df['low'], df['close'], window=14, smooth_window=3)

    # Calculate MACD
    # The 'window_sign' argument was removed in a newer version of the 'ta' library.
    # Use 'signal_period' instead.
    # The parameter name for signal period is `window_sign` instead of `signal_period`
    macd = ta.trend.MACD(df['close'], window_slow=26, window_fast=12, window_sign=9)
    df['macd'] = macd.macd()
    df['macd_signal'] = macd.macd_signal()
    df['macd_diff'] = macd.macd_diff()

    # Calculate RVI
    df['rvi'] = ta.momentum.rsi(df['close'], window=14)  # RVI is often calculated similarly to RSI

    return df

# Function to process the input Excel file
def process_trades(input_file, output_file):
    # Read the input Excel file
    trades_df = pd.read_excel(input_file)

    # Convert Entry Time and Exit Time to datetime
    trades_df['Entry Time'] = pd.to_datetime(trades_df['Entry Time'], format='%d-%m-%Y %H:%M:%S')
    trades_df['Exit Time'] = pd.to_datetime(trades_df['Exit Time'], format='%d-%m-%Y %H:%M:%S')

    # Fetch historical data (replace this with your actual data fetching logic)
    # For demonstration, let's assume we have a DataFrame `historical_data` with OHLCV data
    start_time = trades_df['Entry Time'].min()
    end_time = trades_df['Exit Time'].max()
    historical_data = fetch_data_from_table('eurusdeth30minutes', start_time, end_time)

    historical_data['timestamp'] = pd.to_datetime(historical_data['timestamp'], format='%Y-%m-%d %H:%M:%S')
# Changed the format string to '%Y-%m-%d %H:%M:%S' to match the actual format of the 'timestamp' column
    # Calculate indicators for the historical data
    historical_data = calculate_indicators(historical_data)

    # Initialize an empty DataFrame to store results
    results = []

    # Iterate through each trade
    for index, row in trades_df.iterrows():
        entry_time = row['Entry Time']
        exit_time = row['Exit Time']
        trade_type = row['Trade']

        # Filter historical data for the entry and exit times
        entry_data = historical_data[historical_data['timestamp'] == entry_time]
        exit_data = historical_data[historical_data['timestamp'] == exit_time]

        if not entry_data.empty and not exit_data.empty:
            # Extract indicators for entry and exit
            entry_indicators = entry_data[['ema7', 'ema14', 'ema20', 'ema50', 'ema100', 'ema200', 'rsi', 'mfi', 'smi', 'macd', 'macd_signal', 'macd_diff', 'rvi']].iloc[0]
            exit_indicators = exit_data[['ema7', 'ema14', 'ema20', 'ema50', 'ema100', 'ema200', 'rsi', 'mfi', 'smi', 'macd', 'macd_signal', 'macd_diff', 'rvi']].iloc[0]

            # Combine results
            result = {
                'Trade': trade_type,
                'Entry Time': entry_time,
                'Exit Time': exit_time,
                **{f'Entry {col}': entry_indicators[col] for col in entry_indicators.index},
                **{f'Exit {col}': exit_indicators[col] for col in exit_indicators.index}
            }
            results.append(result)

    # Convert results to a DataFrame
    results_df = pd.DataFrame(results)

    # Save the results to a new Excel file
    results_df.to_excel(output_file, index=False)
    print(f"Results saved to {output_file}")

# Function to generate a list of USA holidays for the last 10 years and the next 20 years
def generate_usa_holidays():
    # Define the start and end years for the range
    current_year = pd.Timestamp.now().year
    start_year = current_year - 10
    end_year = current_year
    # end_year = current_year + 20

    # Initialize an empty list to store holidays
    usa_holidays = []

    # Iterate over the range of years
    for year in range(start_year, end_year + 1):
        # Add New Year's Day
        usa_holidays.append(f'{year}-01-01')
        # Add Martin Luther King Jr. Day (third Monday of January)
        usa_holidays.append(pd.date_range(start=f'{year}-01-01', end=f'{year}-01-31', freq='W-MON')[2])
        # Add Presidents' Day (third Monday of February)
        usa_holidays.append(pd.date_range(start=f'{year}-02-01', end=f'{year}-02-28', freq='W-MON')[2])
        # Add Memorial Day (last Monday of May)
        usa_holidays.append(pd.date_range(start=f'{year}-05-25', end=f'{year}-05-31', freq='W-MON')[-1])
        # Add Independence Day
        usa_holidays.append(f'{year}-07-04')
        # Add Labor Day (first Monday of September)
        usa_holidays.append(pd.date_range(start=f'{year}-09-01', end=f'{year}-09-07', freq='W-MON')[0])
        # Add Thanksgiving Day (fourth Thursday of November)
        thanksgiving_thursdays = pd.date_range(start=f'{year}-11-01', end=f'{year}-11-30', freq='W-THU')
        if len(thanksgiving_thursdays) >= 4:
            usa_holidays.append(thanksgiving_thursdays[3])
        else:
            usa_holidays.append(thanksgiving_thursdays[-1])  # Add the last Thursday if there are fewer than 4
        # Add Christmas Day
        usa_holidays.append(f'{year}-12-25')

    # Convert the list to a pandas DatetimeIndex
    usa_holidays = pd.to_datetime(usa_holidays).date

    return usa_holidays

# Function to calculate the number of trading days in the USA between two dates
def count_trading_days_usa(start_date, end_date):
    # Generate the list of USA holidays
    usa_holidays = generate_usa_holidays()

    # Generate a date range between start and end dates inclusive of both
    date_range = pd.date_range(start=start_date, end=end_date)

    # Exclude weekends (Saturday and Sunday)
    weekdays = date_range[date_range.dayofweek < 5]

    # Exclude holidays observed in the USA market
    trading_days = [day for day in weekdays if day not in usa_holidays]

    # Return the count of trading days
    return len(trading_days)

# Example usage
input_file = '/content/eurusd_30mins_profitable_trades_by_time.xlsx'  # Replace with your input file path
output_file = 'trade_indicators.xlsx'  # Replace with your desired output file path
process_trades(input_file, output_file)

import pandas as pd
import numpy as np

# Load the Excel files
trades_file = "/content/Profitable Trades By Date and Price EURUSD.xlsx"  # File containing trade data
ohlcv_file = "/content/FX_EURUSD, 30_785e6.xlsx"    # File containing OHLCV data

# Read the trade data
trades_df = pd.read_excel(trades_file)

# Read the OHLCV data
ohlcv_df = pd.read_excel(ohlcv_file)

trades_df["Entry Time"] = pd.to_datetime(trades_df["Entry Time"])
trades_df["Exit Time"] = pd.to_datetime(trades_df["Exit Time"])

# Force 'time' column to datetime, handling errors
ohlcv_df["timestamp"] = pd.to_datetime(ohlcv_df["time"], errors='coerce')

# Compute EMA (Exponential Moving Average)
def ema(series, period):
    return series.ewm(span=period, adjust=False).mean()

ohlcv_df["EMA_15"] = ema(ohlcv_df["close"], 15)
ohlcv_df["EMA_30"] = ema(ohlcv_df["close"], 30)

# Compute RSI (Relative Strength Index)
def rsi(series, period=14):
    delta = series.diff(1)
    gain = np.where(delta > 0, delta, 0)
    loss = np.where(delta < 0, -delta, 0)

    avg_gain = pd.Series(gain).rolling(window=period, min_periods=1).mean()
    avg_loss = pd.Series(loss).rolling(window=period, min_periods=1).mean()

    rs = avg_gain / (avg_loss + 1e-10)  # Avoid division by zero
    return 100 - (100 / (1 + rs))

ohlcv_df["RSI_14"] = rsi(ohlcv_df["close"], 14)

# Compute Stochastic Momentum Index (SMI)
def stochastic_momentum_index(df, period=5):
    highest_high = df["high"].rolling(window=period, min_periods=1).max()
    lowest_low = df["low"].rolling(window=period, min_periods=1).min()
    smi = ((df["close"] - lowest_low) / (highest_high - lowest_low + 1e-10)) * 100  # Avoid division by zero
    return smi

ohlcv_df["SMI"] = stochastic_momentum_index(ohlcv_df, 5)

# Compute EMA 20 of RSI
ohlcv_df["EMA_20_RSI"] = ema(ohlcv_df["RSI_14"], 20)

# Merge indicators with trade data based on entry time
trades_df = trades_df.merge(ohlcv_df[["timestamp", "EMA_15", "EMA_30", "RSI_14", "SMI", "EMA_20_RSI"]],
                            left_on="Entry Time", right_on="timestamp", how="left")

# Drop the duplicate timestamp column
trades_df.drop(columns=["timestamp"], inplace=True)

# Save the updated trades file
output_file = "updated_trades_no_talib.xlsx"
trades_df.to_excel(output_file, index=False)

print(f"Updated trades file saved as {output_file}")

